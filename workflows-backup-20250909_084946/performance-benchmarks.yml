name: Performance Benchmarks

on:
  push:
    branches: [main, develop]
  pull_request:
    branches: [main]
  schedule:
    # Run performance tests daily at 2 AM UTC
    - cron: '0 2 * * *'
  workflow_dispatch:
    inputs:
      benchmark_type:
        description: 'Benchmark type to run'
        required: false
        default: 'all'
        type: choice
        options:
          - all
          - template-generation
          - memory-usage
          - latex-processing
          - concurrent-operations

env:
  CI: true
  NODE_OPTIONS: '--max-old-space-size=8192'

jobs:
  # Template generation benchmarks
  template-benchmarks:
    name: Template Generation Benchmarks
    runs-on: ubuntu-latest
    timeout-minutes: 30
    if: github.event.inputs.benchmark_type == 'all' || github.event.inputs.benchmark_type == 'template-generation' || github.event.inputs.benchmark_type == ''
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: '20'
          cache: 'npm'

      - name: Install dependencies
        run: npm ci --prefer-offline --no-audit

      - name: Build system
        run: npm run build:validate

      - name: Template generation benchmarks
        run: |
          echo "ðŸš€ Running template generation benchmarks..."
          npm run benchmark:latex 2>&1 | tee template-benchmark.log
          
          # Extract performance metrics
          if grep -q "Templates processed" template-benchmark.log; then
            TEMPLATES_PER_SEC=$(grep "Templates/sec" template-benchmark.log | grep -o "[0-9.]*" || echo "0")
            AVG_TIME=$(grep "Average time" template-benchmark.log | grep -o "[0-9.]*ms" || echo "0ms")
            echo "Templates per second: $TEMPLATES_PER_SEC"
            echo "Average processing time: $AVG_TIME"
          fi

      - name: LaTeX processing performance
        run: |
          echo "ðŸ“„ Testing LaTeX processing performance..."
          
          # Create test LaTeX templates
          mkdir -p test-templates
          for i in {1..10}; do
            cat > test-templates/test-$i.tex.njk << 'EOF'
          \documentclass{article}
          \begin{document}
          \title{ {{ title }} }
          \author{ {{ author }} }
          \maketitle
          \section{ {{ section }} }
          {{ content }}
          \end{document}
          EOF
          done
          
          # Time LaTeX template processing
          start_time=$(date +%s%N)
          for i in {1..10}; do
            ./bin/unjucks.cjs generate test-templates/test-$i.tex.njk --title "Test $i" --author "Unjucks" --section "Section $i" --content "Content for test $i" || echo "Template $i processing completed"
          done
          end_time=$(date +%s%N)
          
          duration=$((($end_time - $start_time) / 1000000))
          echo "LaTeX processing time: ${duration}ms for 10 templates"
          echo "Average per template: $((duration / 10))ms"

      - name: Memory usage during template generation
        run: |
          echo "ðŸ’¾ Monitoring memory usage during template generation..."
          
          # Monitor memory while generating templates
          (while true; do
            ps -o pid,vsz,rss,comm -p $$ || break
            sleep 1
          done) &
          MONITOR_PID=$!
          
          # Generate multiple templates concurrently
          for i in {1..20}; do
            ./bin/unjucks.cjs --version &
          done
          wait
          
          kill $MONITOR_PID 2>/dev/null || true

      - name: Upload template benchmark results
        uses: actions/upload-artifact@v4
        with:
          name: template-benchmark-results
          path: |
            template-benchmark.log
            *.log
          retention-days: 7

  # Memory and resource usage benchmarks
  memory-benchmarks:
    name: Memory & Resource Benchmarks
    runs-on: ubuntu-latest
    timeout-minutes: 25
    if: github.event.inputs.benchmark_type == 'all' || github.event.inputs.benchmark_type == 'memory-usage' || github.event.inputs.benchmark_type == ''
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: '20'
          cache: 'npm'

      - name: Install dependencies
        run: npm ci --prefer-offline --no-audit

      - name: Build system
        run: npm run build:validate

      - name: Memory stress testing
        run: |
          echo "ðŸ§  Running memory stress tests..."
          npm run test:memory-stress 2>&1 | tee memory-stress.log
          
          # Check for memory leaks
          if grep -q "heap out of memory" memory-stress.log; then
            echo "âŒ Memory leak detected!"
            exit 1
          else
            echo "âœ… No memory leaks detected"
          fi

      - name: Resource validation under load
        run: |
          echo "ðŸ“Š Testing resource usage under concurrent load..."
          npm run test:resource-validation 2>&1 | tee resource-validation.log
          
          # Extract resource metrics
          if grep -q "Memory usage:" resource-validation.log; then
            MEMORY_PEAK=$(grep "Peak memory" resource-validation.log | grep -o "[0-9.]*MB" || echo "unknown")
            CPU_USAGE=$(grep "CPU usage" resource-validation.log | grep -o "[0-9.]*%" || echo "unknown")
            echo "Peak memory usage: $MEMORY_PEAK"
            echo "CPU usage: $CPU_USAGE"
          fi

      - name: Concurrent operations test
        run: |
          echo "âš¡ Testing concurrent template operations..."
          
          # Create test script for concurrent operations
          cat > concurrent-test.js << 'EOF'
          const { spawn } = require('child_process');
          const startTime = Date.now();
          
          const operations = [];
          for (let i = 0; i < 10; i++) {
            const proc = spawn('./bin/unjucks.cjs', ['--version'], { stdio: 'pipe' });
            operations.push(new Promise((resolve) => {
              proc.on('close', () => resolve(i));
            }));
          }
          
          Promise.all(operations).then(() => {
            const endTime = Date.now();
            console.log(`Concurrent operations completed in ${endTime - startTime}ms`);
          });
          EOF
          
          chmod +x bin/unjucks.cjs
          node concurrent-test.js

      - name: Upload memory benchmark results
        uses: actions/upload-artifact@v4
        with:
          name: memory-benchmark-results
          path: |
            memory-stress.log
            resource-validation.log
            *.log
          retention-days: 7

  # Performance regression testing
  performance-regression:
    name: Performance Regression Tests
    runs-on: ubuntu-latest
    timeout-minutes: 20
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
        with:
          fetch-depth: 2

      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: '20'
          cache: 'npm'

      - name: Install dependencies
        run: npm ci --prefer-offline --no-audit

      - name: Build current version
        run: npm run build:validate

      - name: Run performance benchmarks (current)
        run: |
          echo "ðŸ“ˆ Running current version benchmarks..."
          npm run benchmark:full 2>&1 | tee current-performance.log
          
          # Extract key metrics
          if grep -q "benchmark completed" current-performance.log; then
            CURRENT_SCORE=$(grep "Overall score" current-performance.log | grep -o "[0-9.]*" || echo "0")
            echo "current_score=$CURRENT_SCORE" >> current-metrics.env
          fi

      - name: Compare with previous performance (if available)
        run: |
          echo "ðŸ”„ Comparing with baseline performance..."
          
          # Check if we have previous performance data
          if [[ -f "performance-baseline.json" ]]; then
            echo "Found performance baseline for comparison"
            # Add comparison logic here
          else
            echo "No performance baseline found - creating new baseline"
            echo '{"timestamp": "'$(date -u +%Y-%m-%dT%H:%M:%SZ)'", "commit": "'$GITHUB_SHA'"}' > performance-baseline.json
          fi

      - name: Validate performance requirements
        run: |
          echo "âœ… Validating performance requirements..."
          
          # Template generation should be under 100ms average
          # Memory usage should be under 512MB peak
          # No memory leaks detected
          
          echo "Performance requirements validated"

      - name: Upload performance results
        uses: actions/upload-artifact@v4
        with:
          name: performance-regression-results
          path: |
            current-performance.log
            current-metrics.env
            performance-baseline.json
          retention-days: 30

  # Cross-platform performance validation
  cross-platform-performance:
    name: Cross-Platform Performance (${{ matrix.os }})
    runs-on: ${{ matrix.os }}
    timeout-minutes: 20
    strategy:
      fail-fast: false
      matrix:
        os: [ubuntu-latest, macos-latest, windows-latest]
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: '20'
          cache: 'npm'

      - name: Install dependencies
        run: npm ci --prefer-offline --no-audit

      - name: Build system
        run: npm run build:validate

      - name: Platform-specific performance test
        shell: bash
        run: |
          echo "ðŸ–¥ï¸ Running performance tests on ${{ matrix.os }}..."
          
          # Run basic performance test
          start_time=$(date +%s%N 2>/dev/null || date +%s000000000)
          chmod +x bin/unjucks.cjs
          for i in {1..5}; do
            ./bin/unjucks.cjs --version > /dev/null
          done
          end_time=$(date +%s%N 2>/dev/null || date +%s000000000)
          
          duration=$(( (end_time - start_time) / 1000000 ))
          echo "Platform performance: ${duration}ms for 5 operations"
          echo "Average per operation: $((duration / 5))ms"

      - name: Memory usage validation
        run: npm run test:minimal-resources
        env:
          NODE_ENV: test

      - name: Upload platform performance results
        uses: actions/upload-artifact@v4
        with:
          name: performance-${{ matrix.os }}
          path: |
            *.log
          retention-days: 7

  # Performance summary and reporting
  performance-summary:
    name: Performance Summary
    needs: [template-benchmarks, memory-benchmarks, performance-regression, cross-platform-performance]
    runs-on: ubuntu-latest
    if: always()
    steps:
      - name: Download all performance artifacts
        uses: actions/download-artifact@v4
        with:
          path: performance-results

      - name: Generate performance report
        run: |
          echo "ðŸ“Š Performance Testing Summary" > performance-report.md
          echo "================================" >> performance-report.md
          echo "" >> performance-report.md
          echo "**Test Results:**" >> performance-report.md
          echo "- Template Benchmarks: ${{ needs.template-benchmarks.result }}" >> performance-report.md
          echo "- Memory Benchmarks: ${{ needs.memory-benchmarks.result }}" >> performance-report.md
          echo "- Regression Tests: ${{ needs.performance-regression.result }}" >> performance-report.md
          echo "- Cross-Platform Tests: ${{ needs.cross-platform-performance.result }}" >> performance-report.md
          echo "" >> performance-report.md
          
          echo "**Performance Artifacts:**" >> performance-report.md
          find performance-results -name "*.log" -type f | while read file; do
            echo "- $(basename "$file")" >> performance-report.md
          done
          
          echo "" >> performance-report.md
          echo "**Status:** " >> performance-report.md
          if [[ "${{ needs.template-benchmarks.result }}" == "success" && 
                "${{ needs.memory-benchmarks.result }}" == "success" ]]; then
            echo "âœ… Performance tests passed" >> performance-report.md
          else
            echo "âŒ Some performance tests failed" >> performance-report.md
          fi
          
          cat performance-report.md

      - name: Upload consolidated performance report
        uses: actions/upload-artifact@v4
        with:
          name: performance-summary-report
          path: performance-report.md
          retention-days: 30