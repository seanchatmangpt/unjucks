import { defineCommand } from 'citty';
import { consola } from 'consola';
import { promises as fs } from 'fs';
import { join, resolve } from 'path';
import { KnowledgeGraphPipeline } from '../lib/knowledge-graph/kg-pipeline.js';
import { performance } from 'perf_hooks';

export default defineCommand({
  meta: {
    name: 'knowledge-graph',
    description: 'Generate comprehensive knowledge graphs from structured data using semantic web standards'
  },
  subCommands: {
    generate: defineCommand({
      meta: {
        name: 'generate',
        description: 'Generate knowledge graph RDF files from input data'
      },
      args: {
        input: {
          type: 'string',
          description: 'Input data file (JSON)',
          required: true
        },
        output: {
          type: 'string', 
          description: 'Output directory for generated RDF files',
          default: './kg-output'
        },
        templates: {
          type: 'string',
          description: 'Templates directory',
          default: './tests/fixtures/knowledge-graphs/templates'
        },
        format: {
          type: 'string',
          description: 'RDF serialization format',
          default: 'turtle',
          valueHint: 'turtle|rdfxml|jsonld|ntriples'
        },
        validate: {
          type: 'boolean',
          description: 'Enable SHACL validation',
          default: true
        },
        force: {
          type: 'boolean',
          description: 'Force overwrite existing files',
          default: false
        },
        report: {
          type: 'boolean',
          description: 'Generate quality report',
          default: true
        }
      },
      async run({ args }) {
        const startTime = performance.now();
        
        try {
          // Validate input file
          const inputPath = resolve(args.input);
          await fs.access(inputPath);
          
          // Load dataset
          consola.info('Loading dataset from:', inputPath);
          const datasetContent = await fs.readFile(inputPath, 'utf8');
          const dataset = JSON.parse(datasetContent);
          
          // Validate templates directory
          const templatesDir = resolve(args.templates);
          await fs.access(templatesDir);
          
          // Initialize pipeline
          const pipeline = new KnowledgeGraphPipeline({
            templatesDir,
            outputDir: resolve(args.output),
            validate: args.validate,
            force: args.force,
            format: args.format
          });
          
          consola.info(`Generating knowledge graph for domain: ${dataset.domain}`);
          consola.info(`Output directory: ${args.output}`);
          consola.info(`Templates directory: ${args.templates}`);
          
          // Generate knowledge graph
          const result = await pipeline.generateKnowledgeGraph(dataset);
          
          const processingTime = performance.now() - startTime;
          
          // Display results
          consola.success('Knowledge graph generation completed!');
          consola.info(`Generated files: ${result.files.length}`);
          consola.info(`Total triples: ${result.triples.toLocaleString()}`);
          consola.info(`Total entities: ${result.entities.toLocaleString()}`);
          consola.info(`Processing time: ${Math.round(processingTime)}ms`);
          
          // Display generated files
          for (const file of result.files) {
            const relativePath = file.replace(resolve(args.output), '');
            consola.log(`  ðŸ“„ ${relativePath}`);
          }
          
          // Display validation results
          if (result.validationResult) {
            const { valid, errors, warnings, statistics } = result.validationResult;
            
            if (valid) {
              consola.success('âœ… Validation passed');
            } else {
              consola.warn(`âš ï¸  Validation failed with ${errors.length} errors`);
            }
            
            if (warnings.length > 0) {
              consola.warn(`âš ï¸  ${warnings.length} warnings found`);
            }
            
            consola.info(`Validation time: ${statistics.validationTime}ms`);
          }
          
          // Generate quality report
          if (args.report && result.validationResult) {
            consola.info('Generating quality report...');
            const reportPath = await pipeline.generateQualityReport(dataset, result.validationResult);
            const relativeReportPath = reportPath.replace(resolve(args.output), '');
            consola.success(`ðŸ“Š Quality report: ${relativeReportPath}`);
          }
          
          // Format conversion
          if (args.format !== 'turtle') {
            consola.info(`Converting to ${args.format} format...`);
            for (const file of result.files.filter(f => f.endsWith('.ttl'))) {
              await pipeline.convertFormat(file, args.format);
            }
            consola.success(`Format conversion completed`);
          }
          
          consola.box(`Knowledge Graph Generated Successfully!
            
Domain: ${dataset.domain}
Files: ${result.files.length}
Triples: ${result.triples.toLocaleString()}
Entities: ${result.entities.toLocaleString()}
Time: ${Math.round(processingTime)}ms
Output: ${args.output}`);
          
        } catch (error) {
          consola.error('Knowledge graph generation failed:', error.message);
          process.exit(1);
        }
      }
    }),
    
    validate: defineCommand({
      meta: {
        name: 'validate',
        description: 'Validate knowledge graph RDF files against SHACL constraints'
      },
      args: {
        input: {
          type: 'string',
          description: 'RDF files directory or single file',
          required: true
        },
        shapes: {
          type: 'string',
          description: 'SHACL shapes file',
          default: './tests/fixtures/knowledge-graphs/schemas/kg-validation.shacl.ttl'
        },
        report: {
          type: 'string',
          description: 'Output validation report file',
          default: './validation-report.json'
        }
      },
      async run({ args }) {
        try {
          consola.info('Validating knowledge graph files...');
          
          const inputPath = resolve(args.input);
          const shapesPath = resolve(args.shapes);
          
          // Check if input is file or directory
          const inputStat = await fs.stat(inputPath);
          let rdfFiles = [];
          
          if (inputStat.isDirectory()) {
            const files = await fs.readdir(inputPath, { recursive: true });
            rdfFiles = files
              .filter((file) => typeof file === 'string' && (file.endsWith('.ttl') || file.endsWith('.rdf') || file.endsWith('.nt')))
              .map((file) => join(inputPath, file));
          } else {
            rdfFiles = [inputPath];
          }
          
          consola.info(`Found ${rdfFiles.length} RDF files to validate`);
          
          // Initialize pipeline for validation
          const pipeline = new KnowledgeGraphPipeline({
            templatesDir: '',
            outputDir: '',
            validate: true,
            shaclValidation: true
          });
          
          // Validate files
          const validationResult = await pipeline.validateKnowledgeGraph(rdfFiles);
          
          // Generate report
          const report = {
            timestamp: this.getDeterministicDate().toISOString(),
            inputFiles: rdfFiles.length,
            validation: validationResult,
            files: rdfFiles.map(file => ({
              path: file,
              size: (await fs.stat(file)).size
            }))
          };
          
          await fs.writeFile(resolve(args.report), JSON.stringify(report, null, 2));
          
          // Display results
          if (validationResult.valid) {
            consola.success('âœ… All files passed validation');
          } else {
            consola.error(`âŒ Validation failed with ${validationResult.errors.length} errors`);
            validationResult.errors.forEach(error => consola.error(`  â€¢ ${error}`));
          }
          
          if (validationResult.warnings.length > 0) {
            consola.warn(`âš ï¸  ${validationResult.warnings.length} warnings found`);
            validationResult.warnings.forEach(warning => consola.warn(`  â€¢ ${warning}`));
          }
          
          consola.info(`Total triples: ${validationResult.statistics.totalTriples.toLocaleString()}`);
          consola.info(`Total entities: ${validationResult.statistics.totalEntities.toLocaleString()}`);
          consola.info(`Validation time: ${validationResult.statistics.validationTime}ms`);
          consola.info(`Report saved to: ${args.report}`);
          
        } catch (error) {
          consola.error('Validation failed:', error.message);
          process.exit(1);
        }
      }
    }),
    
    query: defineCommand({
      meta: {
        name: 'query',
        description: 'Execute SPARQL queries against knowledge graph'
      },
      args: {
        endpoint: {
          type: 'string',
          description: 'SPARQL endpoint URL',
          required: true
        },
        query: {
          type: 'string',
          description: 'SPARQL query string or file path'
        },
        file: {
          type: 'string',
          description: 'SPARQL query file'
        },
        format: {
          type: 'string',
          description: 'Output format',
          default: 'table',
          valueHint: 'table|json|csv|tsv'
        },
        output: {
          type: 'string',
          description: 'Output file (optional)'
        }
      },
      async run({ args }) {
        try {
          let queryText;
          
          if (args.query) {
            queryText = args.query;
          } else if (args.file) {
            const queryPath = resolve(args.file);
            queryText = await fs.readFile(queryPath, 'utf8');
          } else {
            throw new Error('Either --query or --file must be specified');
          }
          
          consola.info('Executing SPARQL query...');
          consola.info(`Endpoint: ${args.endpoint}`);
          
          // This would integrate with actual SPARQL client library
          // For now, provide a placeholder implementation
          const mockResults = [
            { entity: 'http://example.org/kg/person-001', name: 'Alice Johnson', type: 'Person' },
            { entity: 'http://example.org/kg/person-002', name: 'Bob Smith', type: 'Person' }
          ];
          
          const startTime = performance.now();
          // const results = await executeSparqlQuery(args.endpoint, queryText);
          const results = mockResults;
          const queryTime = performance.now() - startTime;
          
          consola.success(`Query completed in ${Math.round(queryTime)}ms`);
          consola.info(`Results: ${results.length} rows`);
          
          // Format and display results
          if (args.format === 'json') {
            const jsonOutput = JSON.stringify(results, null, 2);
            if (args.output) {
              await fs.writeFile(resolve(args.output), jsonOutput);
              consola.success(`Results saved to: ${args.output}`);
            } else {
              console.log(jsonOutput);
            }
          } else if (args.format === 'table') {
            console.table(results);
          } else if (args.format === 'csv') {
            if (results.length > 0) {
              const headers = Object.keys(results[0]);
              const csvContent = [
                headers.join(','),
                ...results.map(row => headers.map(h => row[h] || '').join(','))
              ].join('\n');
              
              if (args.output) {
                await fs.writeFile(resolve(args.output), csvContent);
                consola.success(`Results saved to: ${args.output}`);
              } else {
                console.log(csvContent);
              }
            }
          }
          
        } catch (error) {
          consola.error('Query execution failed:', error.message);
          process.exit(1);
        }
      }
    }),
    
    deploy: defineCommand({
      meta: {
        name: 'deploy',
        description: 'Deploy knowledge graph with triple store infrastructure'
      },
      args: {
        config: {
          type: 'string',
          description: 'Deployment configuration file',
          default: './tests/fixtures/knowledge-graphs/deployment/docker-compose.yml'
        },
        env: {
          type: 'string',
          description: 'Environment (development|staging|production)',
          default: 'development'
        },
        data: {
          type: 'string',
          description: 'Knowledge graph data directory',
          required: true
        },
        profiles: {
          type: 'string',
          description: 'Docker Compose profiles to activate (comma-separated)',
          default: 'default'
        }
      },
      async run({ args }) {
        try {
          consola.info('Deploying knowledge graph infrastructure...');
          
          const configPath = resolve(args.config);
          const dataPath = resolve(args.data);
          
          // Validate configuration and data
          await fs.access(configPath);
          await fs.access(dataPath);
          
          consola.info(`Configuration: ${configPath}`);
          consola.info(`Data directory: ${dataPath}`);
          consola.info(`Environment: ${args.env}`);
          consola.info(`Profiles: ${args.profiles}`);
          
          // Set environment variables
          const env = {
            ...process.env,
            KG_DATA_PATH: dataPath,
            KG_ENV: args.env,
            API_KEY: process.env.API_KEY || 'kg-api-key-' + this.getDeterministicTimestamp(),
            BACKUP_S3_BUCKET: process.env.BACKUP_S3_BUCKET || '',
            BACKUP_S3_ACCESS_KEY: process.env.BACKUP_S3_ACCESS_KEY || '',
            BACKUP_S3_SECRET_KEY: process.env.BACKUP_S3_SECRET_KEY || ''
          };
          
          // Execute Docker Compose
          const { spawn } = await import('child_process');
          const profiles = args.profiles.split(',').map(p => p.trim()).join(',');
          
          const dockerArgs = [
            'compose',
            '-f', configPath,
            '--profile', profiles,
            'up', '-d'
          ];
          
          const dockerProcess = spawn('docker', dockerArgs, {
            stdio: 'inherit',
            env,
            cwd: resolve(args.config, '..')
          });
          
          dockerProcess.on('close', (code) => {
            if (code === 0) {
              consola.success('Knowledge graph infrastructure deployed successfully!');
              consola.info('\nðŸš€ Available Services:');
              consola.info('  â€¢ SPARQL Endpoint: http://localhost:3030/ds/sparql');
              consola.info('  â€¢ Knowledge Graph API: http://localhost:3000');
              consola.info('  â€¢ Web Interface: http://localhost:3001');
              consola.info('  â€¢ Query Interface: http://localhost:3002');
              
              if (profiles.includes('monitoring')) {
                consola.info('  â€¢ Prometheus: http://localhost:9090');
                consola.info('  â€¢ Grafana: http://localhost:3003 (admin/admin123)');
              }
              
              consola.info('\nðŸ“š Next Steps:');
              consola.info('  1. Load data: docker compose --profile loader up kg-loader');
              consola.info('  2. Test API: curl http://localhost:3000/health');
              consola.info('  3. Run queries: unjucks knowledge-graph query --endpoint http://localhost:3030/ds/sparql --file queries.sparql');
            } else {
              consola.error(`Docker Compose exited with code ${code}`);
              process.exit(1);
            }
          });
          
        } catch (error) {
          consola.error('Deployment failed:', error.message);
          process.exit(1);
        }
      }
    }),
    
    convert: defineCommand({
      meta: {
        name: 'convert',
        description: 'Convert knowledge graph between different RDF formats'
      },
      args: {
        input: {
          type: 'string',
          description: 'Input RDF file or directory',
          required: true
        },
        output: {
          type: 'string',
          description: 'Output file or directory',
          required: true
        },
        from: {
          type: 'string',
          description: 'Source format',
          default: 'turtle',
          valueHint: 'turtle|rdfxml|jsonld|ntriples'
        },
        to: {
          type: 'string',
          description: 'Target format',
          required: true,
          valueHint: 'turtle|rdfxml|jsonld|ntriples'
        }
      },
      async run({ args }) {
        try {
          consola.info(`Converting from ${args.from} to ${args.to}...`);
          
          const inputPath = resolve(args.input);
          const outputPath = resolve(args.output);
          
          // Initialize pipeline
          const pipeline = new KnowledgeGraphPipeline({
            templatesDir: '',
            outputDir: '',
            format: args.to
          });
          
          // Check if input is file or directory
          const inputStat = await fs.stat(inputPath);
          
          if (inputStat.isDirectory()) {
            // Convert directory of files
            const files = await fs.readdir(inputPath, { recursive: true });
            const rdfFiles = files.filter((file) => 
              typeof file === 'string' && file.endsWith(getFileExtension(args.from))
            );
            
            await fs.mkdir(outputPath, { recursive: true });
            
            for (const file of rdfFiles) {
              const inputFile = join(inputPath, file);
              const outputFile = await pipeline.convertFormat(inputFile, args.to);
              consola.info(`Converted: ${file} â†’ ${outputFile}`);
            }
            
            consola.success(`Converted ${rdfFiles.length} files`);
            
          } else {
            // Convert single file
            const outputFile = await pipeline.convertFormat(inputPath, args.to);
            consola.success(`Converted: ${inputPath} â†’ ${outputFile}`);
          }
          
        } catch (error) {
          consola.error('Format conversion failed:', error.message);
          process.exit(1);
        }
      }
    })
  }
});

function getFileExtension(format) {
  const extensions = {
    turtle: '.ttl',
    rdfxml: '.rdf',
    jsonld: '.jsonld',
    ntriples: '.nt'
  };
  return extensions[format] || '.ttl';
}