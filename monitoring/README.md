# Unjucks Monitoring Stack\n\nA comprehensive observability solution providing structured logging, distributed tracing, custom metrics, alerting, SLI/SLO tracking, and health monitoring.\n\n## Features\n\n### 🔍 Structured Logging\n- **Correlation IDs**: Automatic request correlation across services\n- **Multiple Outputs**: Console, file, syslog, Elasticsearch\n- **Security**: Automatic sanitization of sensitive data\n- **Contextual**: Rich metadata and structured fields\n\n### 🔗 Distributed Tracing\n- **OpenTelemetry**: Industry-standard tracing with Jaeger/Zipkin export\n- **Auto-instrumentation**: HTTP, Express, database, Redis\n- **Performance**: Configurable sampling rates\n- **Context Propagation**: Automatic trace context passing\n\n### 📊 Custom Metrics\n- **Prometheus**: Native Prometheus metrics export\n- **Business Metrics**: Track user actions, file generations, API calls\n- **Performance**: Histograms for latency, counters for events\n- **Resource Monitoring**: CPU, memory, disk usage\n\n### 🚨 Alerting System\n- **Multi-Channel**: Email, Slack, PagerDuty, webhooks\n- **Escalation**: Configurable escalation policies\n- **Suppression**: Prevent alert fatigue\n- **Smart Routing**: Severity-based channel selection\n\n### 📈 SLI/SLO Tracking\n- **Error Budgets**: Track and alert on error budget burn\n- **Availability**: 99.9% uptime SLO with compliance tracking\n- **Latency**: 95% requests under 500ms\n- **Custom SLIs**: Business-specific indicators\n\n### 💓 Health Monitoring\n- **Endpoint Checks**: HTTP, database, Redis, filesystem\n- **Resource Thresholds**: Memory, CPU, disk usage\n- **Auto-Recovery**: Configurable recovery mechanisms\n- **Dependency Tracking**: External service monitoring\n\n## Quick Start\n\n```javascript\nconst { createMonitoringStack } = require('./monitoring');\n\n// Initialize monitoring\nconst monitoring = createMonitoringStack({\n  environment: 'production'\n});\n\nawait monitoring.initialize();\n\n// Add to Express app\napp.use(monitoring.createMiddleware());\napp.get('/health', monitoring.createHealthEndpoint());\napp.get('/metrics', monitoring.createMetricsEndpoint());\n```\n\n## Configuration\n\n### Environment Variables\n\n```bash\n# Service Configuration\nSERVICE_NAME=unjucks-service\nSERVICE_VERSION=1.0.0\nNODE_ENV=production\n\n# Logging\nLOG_LEVEL=info\nSYSLOG_HOST=localhost\nELASTICSEARCH_HOST=localhost:9200\n\n# Tracing\nJAEGER_ENDPOINT=http://localhost:14268/api/traces\nTRACING_SAMPLING_RATE=0.1\n\n# Alerting\nSMTP_HOST=smtp.gmail.com\nSMTP_USER=alerts@company.com\nSMTP_PASS=password\nALERT_TO_EMAIL=team@company.com\nSLACK_WEBHOOK_URL=https://hooks.slack.com/...\nPAGERDUTY_INTEGRATION_KEY=your-key\n\n# Health Checks\nDATABASE_URL=postgresql://...\nREDIS_HOST=localhost\nREDIS_PORT=6379\n```\n\n### Custom Configuration\n\n```javascript\nconst MonitoringConfig = require('./monitoring/config/monitoring-config');\n\nconst config = new MonitoringConfig('production');\nconfig.override({\n  alerting: {\n    rules: {\n      customRule: {\n        enabled: true,\n        threshold: 0.95,\n        timeWindow: '10m',\n        severity: 'high'\n      }\n    }\n  }\n});\n\nconst monitoring = createMonitoringStack({ config });\n```\n\n## Usage Examples\n\n### Structured Logging\n\n```javascript\nconst { StructuredLogger } = require('./monitoring');\nconst logger = new StructuredLogger();\n\n// Business events\nlogger.logEvent('user_registration', {\n  userId: '12345',\n  email: 'user@example.com',\n  plan: 'premium'\n});\n\n// Performance tracking\nlogger.logPerformance('template_render', 250, {\n  templateName: 'user-profile',\n  variables: 15\n});\n\n// Security events\nlogger.logSecurity('login_attempt', 'user123', '/login', 'success');\n```\n\n### Distributed Tracing\n\n```javascript\nconst { DistributedTracer } = require('./monitoring');\nconst tracer = new DistributedTracer();\n\n// Automatic tracing with Express\napp.use(tracer.middleware());\n\n// Manual span creation\nawait tracer.withSpan('file_generation', async () => {\n  // Your code here\n  tracer.addSpanAttributes({\n    'file.type': 'component',\n    'template.name': 'react-component'\n  });\n});\n```\n\n### Custom Metrics\n\n```javascript\nconst { CustomMetrics } = require('./monitoring');\nconst metrics = new CustomMetrics();\n\n// Record business events\nmetrics.recordBusinessEvent('file_generated', 'success', 'premium');\n\n// Track operations\nconst timer = metrics.createTimer('operation');\n// ... do work ...\ntimer.end({ operation: 'template_render', status: 'success' });\n\n// Manual metrics\nmetrics.recordHttpRequest('POST', 200, '/api/generate', 0.5, 1024, 2048);\n```\n\n### SLI/SLO Tracking\n\n```javascript\nconst { SLISLOTracker } = require('./monitoring');\nconst sliSlo = new SLISLOTracker();\n\n// Record SLI measurements\nsliSlo.recordSLI('availability', {\n  successfulRequests: 995,\n  failedRequests: 5\n});\n\nsliSlo.recordSLI('latency', {\n  requestsWithinThreshold: 950,\n  totalRequests: 1000\n});\n\n// Check SLO status\nconst status = sliSlo.getSLOStatus('availability_slo');\nconsole.log('Availability SLO:', status.evaluation.compliance);\n```\n\n### Health Monitoring\n\n```javascript\nconst { HealthMonitor } = require('./monitoring');\nconst health = new HealthMonitor();\n\n// Add custom health check\nhealth.addCheck('custom_service', {\n  name: 'Custom Service Health',\n  type: 'custom',\n  config: {\n    checkFunction: async () => {\n      // Your custom check logic\n      return { status: 'healthy', responseTime: 50 };\n    }\n  },\n  interval: 30000,\n  critical: true\n});\n\n// Get health status\nconst status = health.getOverallHealth();\nconsole.log('Service health:', status.status);\n```\n\n### Alert Management\n\n```javascript\nconst { AlertManager } = require('./monitoring');\nconst alerts = new AlertManager();\n\n// Trigger custom alert\nalerts.emit('alert', {\n  name: 'High Template Generation Rate',\n  description: 'Template generation rate exceeded normal limits',\n  severity: 'medium',\n  source: 'business-logic',\n  tags: ['performance', 'templates'],\n  metadata: {\n    currentRate: 1000,\n    threshold: 500\n  }\n});\n\n// Add suppression rule\nalerts.addSuppressionRule({\n  alertName: 'High Template Generation Rate',\n  maxFrequency: {\n    window: 3600000, // 1 hour\n    count: 2\n  }\n});\n```\n\n## Dashboards\n\n### Grafana Integration\n\nThe monitoring stack includes pre-built Grafana dashboards:\n\n1. **Service Overview**: High-level health and performance metrics\n2. **SLI/SLO Dashboard**: Service level objectives and error budgets\n3. **Alerts Dashboard**: Alert status and incident tracking\n4. **Business Metrics**: Application-specific KPIs\n\nImport dashboards from `monitoring/dashboards/grafana-dashboards.json`.\n\n### Prometheus Metrics\n\nAccess metrics at `/metrics` endpoint:\n\n```\n# HELP unjucks_http_requests_total Total number of HTTP requests\n# TYPE unjucks_http_requests_total counter\nunjucks_http_requests_total{method=\"GET\",status_code=\"200\",route=\"/api/generate\"} 1543\n\n# HELP unjucks_http_request_duration_seconds HTTP request duration\n# TYPE unjucks_http_request_duration_seconds histogram\nunjucks_http_request_duration_seconds_bucket{method=\"POST\",route=\"/api/generate\",le=\"0.1\"} 892\n```\n\n## Architecture\n\n```\n┌─────────────────┐    ┌─────────────────┐    ┌─────────────────┐\n│  Application    │    │  Monitoring     │    │  External       │\n│                 │    │  Stack          │    │  Systems        │\n│  ┌─────────────┐│    │                 │    │                 │\n│  │ Express App ││────┤ ┌─────────────┐ │────┤ ┌─────────────┐ │\n│  └─────────────┘│    │ │ Structured  │ │    │ │ Grafana     │ │\n│  ┌─────────────┐│    │ │ Logger      │ │    │ │ Dashboard   │ │\n│  │ Business    ││────┤ └─────────────┘ │    │ └─────────────┘ │\n│  │ Logic       ││    │ ┌─────────────┐ │    │ ┌─────────────┐ │\n│  └─────────────┘│    │ │ Distributed │ │────┤ │ Jaeger      │ │\n│                 │    │ │ Tracer      │ │    │ │ Tracing     │ │\n└─────────────────┘    │ └─────────────┘ │    │ └─────────────┘ │\n                       │ ┌─────────────┐ │    │ ┌─────────────┐ │\n                       │ │ Custom      │ │────┤ │ Prometheus  │ │\n                       │ │ Metrics     │ │    │ │ Metrics     │ │\n                       │ └─────────────┘ │    │ └─────────────┘ │\n                       │ ┌─────────────┐ │    │ ┌─────────────┐ │\n                       │ │ Alert       │ │────┤ │ Slack/Email │ │\n                       │ │ Manager     │ │    │ │ PagerDuty   │ │\n                       │ └─────────────┘ │    │ └─────────────┘ │\n                       │ ┌─────────────┐ │    │                 │\n                       │ │ SLI/SLO     │ │    │                 │\n                       │ │ Tracker     │ │    │                 │\n                       │ └─────────────┘ │    │                 │\n                       │ ┌─────────────┐ │    │                 │\n                       │ │ Health      │ │    │                 │\n                       │ │ Monitor     │ │    │                 │\n                       │ └─────────────┘ │    │                 │\n                       └─────────────────┘    └─────────────────┘\n```\n\n## Best Practices\n\n### 1. Correlation IDs\n- Always include correlation IDs in requests\n- Use middleware to automatically generate IDs\n- Propagate IDs through all service calls\n\n### 2. Structured Logging\n- Use consistent log levels (DEBUG, INFO, WARN, ERROR)\n- Include relevant context in log messages\n- Sanitize sensitive data automatically\n\n### 3. Metrics\n- Use histograms for timing data\n- Use counters for event counting\n- Use gauges for current state values\n- Tag metrics appropriately for filtering\n\n### 4. Alerting\n- Set appropriate thresholds based on historical data\n- Use escalation policies for critical alerts\n- Implement alert suppression to prevent fatigue\n- Include runbooks in alert descriptions\n\n### 5. SLI/SLO\n- Choose SLIs that matter to users\n- Set realistic SLO targets (99.9% not 100%)\n- Monitor error budget burn rate\n- Use SLO violations to prioritize engineering work\n\n### 6. Health Checks\n- Include all critical dependencies\n- Set appropriate timeouts and retry logic\n- Use health checks for load balancer decisions\n- Monitor health check response times\n\n## Troubleshooting\n\n### Common Issues\n\n1. **High Memory Usage**\n   - Check metric retention settings\n   - Reduce tracing sampling rate\n   - Monitor for memory leaks in health checks\n\n2. **Missing Traces**\n   - Verify Jaeger endpoint configuration\n   - Check sampling rate settings\n   - Ensure correlation headers are propagated\n\n3. **Alert Fatigue**\n   - Review alert thresholds\n   - Implement suppression rules\n   - Use escalation policies effectively\n\n4. **SLO Violations**\n   - Analyze error budget burn rate\n   - Review recent deployments\n   - Check dependency health\n\n### Debug Mode\n\n```javascript\n// Enable debug logging\nprocess.env.LOG_LEVEL = 'debug';\n\n// Test monitoring stack\nconst monitoring = createMonitoringStack();\nawait monitoring.initialize();\nconst results = await monitoring.test();\nconsole.log('Test results:', results);\n```\n\n## Performance Impact\n\n- **CPU Overhead**: < 5% in production\n- **Memory Overhead**: ~50MB baseline\n- **Network Overhead**: Configurable via sampling rates\n- **Disk Usage**: Configurable log retention\n\n## Security Considerations\n\n- Automatic sanitization of sensitive fields\n- Secure transport for metrics and traces\n- API key authentication for monitoring endpoints\n- Rate limiting on monitoring endpoints\n\n## License\n\nMIT License - see LICENSE file for details.\n